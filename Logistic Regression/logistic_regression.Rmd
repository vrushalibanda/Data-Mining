

#### Use the Default dataset from ISLR
```{r}
library(ISLR)
library(tibble)
library(tidyverse)
library(broom)
library(modelr)
library(caret)
as_tibble(Default)
```
Do partition
```{r}
set.seed(1978)
default_idx <-  sample(nrow(Default), 5000)
default_trn <-  Default[default_idx, ]
default_tst <-  Default[-default_idx, ]
```

### Attempt linear regression
```{r}
default_trn_lm <-  default_trn
default_tst_lm <-  default_tst

```
Since linear regression expects a numeric response variable, we coerce the response to be numeric. (Notice that we also shift the results, as we require 0 and 1, not 1 and 2.) Notice we have also copied the dataset so that we can return the original data with factors later.
```{r}
head(as.numeric(default_trn_lm$default))
default_trn_lm$default <-  as.numeric(default_trn_lm$default) - 1
head(default_trn_lm$default,10)
default_tst_lm$default = as.numeric(default_tst_lm$default) - 1
model_lm <-  lm(default ~ balance, data = default_trn_lm)
```
Everything seems to be working, until we plot the results.
```{r}
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Linear Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
abline(model_lm, lwd = 3, col = "dodgerblue")
```
Two issues arise. First, all of the predicted probabilities are below 0.5. That means, we would classify every observation as a "No". This is certainly possible, but not what we would expect.
```{r}
all(predict(model_lm) < 0.5)
```

The next, and bigger issue, is predicted probabilities less than 0.
```{r}
any(predict(model_lm) < 0)
```

### Logistic regression
predicting defaults as a function of balance
```{r}
model_glm <-  glm(default ~ balance, data = default_trn, family = "binomial")
summary(model_glm)
```
Fitting this model looks very similar to fitting a simple linear regression. Instead of lm() we use glm(). The only other difference is the use of family = "binomial" which indicates that we have a two-class categorical response. Using glm() with family = "gaussian" would perform the usual linear regression.

Deviance is analogous to the sum of squares calculations in linear regression and is a measure of the lack of fit to the data in a logistic regression model. The null deviance represents the difference between a model with only the intercept (which means “no predictors”) and a saturated model (a model with a theoretically perfect fit). The goal is for the model deviance (noted as Residual deviance) to be lower; smaller values indicate better fit. In this respect, the null model provides a baseline upon which to compare predictor models.



Fitted coefficients can be obtained the same way they are obtained with linear regression:
```{r}
coef(model_glm)
```
#### Coefficients can also be assessed using *tidy*
```{r}
tidy(model_glm)
```
Meaning: one-unit increase in balance is associated with an increase in the log odds of default by 0.0054 units.

*We can further interpret the balance coefficient as - for every one dollar increase in monthly balance carried, the odds of the customer defaulting increases by a factor of 1.0055.*
```{r}
exp(coef(model_glm))
```
Many aspects of the coefficient output are similar to those discussed in the linear regression output. For example, we can measure the confidence intervals and accuracy of the coefficient estimates by computing their standard errors. For instance, β1_hat
has a p-value < 2e-16 (see summary above) suggesting a statistically significant relationship between balance carried and the probability of defaulting. We can also use the standard errors to get confidence intervals:
```{r}
confint(model_glm)
```
### Making some predictions
```{r}
predict(model_glm, data.frame(balance = c(1000, 2000)), type = "response")
```
**Here we compare the probability of defaulting based on balances of $1000 and $2000. As you can see as the balance moves from $1000 to $2000 the probability of defaulting increases signficantly, from 0.5% to 58%!**

predict.glm() uses type = "link" by default and returns coefficients for each of the observations:
```{r}
head(predict(model_glm))
head(predict(model_glm, ttype = "link"))
```
**Importantly**, these are not predicted probabilities. To obtain the predicted probabilities:
```{r}
head(predict(model_glm, type = "response"))

```

**Note* that these are probabilities, not classifications. To obtain classifications, we will need to compare to the correct cutoff value with an ifelse() statement.
```{r}
model_glm_pred <-  ifelse(predict(model_glm, type = "link") > 0, "Yes", "No")
# model_glm_pred = ifelse(predict(model_glm, type = "response") > 0.5, "Yes", "No")
```
###Error
Once we have classifications, we can calculate metrics such as the trainging classification error rate:
```{r}
calc_class_err <-  function(actual, predicted) {
  mean(actual != predicted)
}
calc_class_err(actual = default_trn$default, predicted = model_glm_pred)
```

As we saw previously, the table() and confusionMatrix() functions can be used to quickly obtain many more metrics.
```{r}
train_tab = table(predicted = model_glm_pred, actual = default_trn$default)
library(caret)
train_con_mat = confusionMatrix(train_tab, positive = "Yes")
c(train_con_mat$overall["Accuracy"], 
  train_con_mat$byClass["Sensitivity"], 
  train_con_mat$byClass["Specificity"])
```
We could also write a custom function for the error for use with trained logist regression models.
```{r}
get_logistic_error = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  preds = ifelse(probs > cut, pos, neg)
  calc_class_err(actual = data[, res], predicted = preds)
}
```

This function will be useful later when calculating train and test errors for several models at the same time.
```{r}
get_logistic_error(model_glm, data = default_trn, 
                   res = "default", pos = "Yes", neg = "No", cut = 0.5)
```

To see how much better logistic regression is for this task, we create the same plot we used for linear regression.
```{r}
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Logistic Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
curve(predict(model_glm, data.frame(balance = x), type = "response"), 
      add = TRUE, lwd = 3, col = "dodgerblue")
abline(v = -coef(model_glm)[1] / coef(model_glm)[2], lwd = 2)
```
### Regression using qualitative predictors (student: Yes or No)
```{r}
model_student <- glm(default ~ student, family = "binomial", data = default_trn)
tidy(model_student)
predict(model_student, data.frame(student = factor(c("Yes", "No"))), type = "response")
```
*Conclusion*: being a student does no seem to be associated with default; not being student has a small and non-significant effect on the rate of default.

Using the usual formula syntax, it is easy to add or remove complexity from logistic regressions.
```{r}
model_1 <-  glm(default ~ 1, data = default_trn, family = "binomial")
model_2 <-  glm(default ~ ., data = default_trn, family = "binomial")
model_3 <-  glm(default ~ . ^ 2 + I(balance ^ 2),
              data = default_trn, family = "binomial")
summary(model_2)
plot(model_2,which=1) ### Deviance residuals have to be on average 0. This is utterly uninformative! 
```
In the case of multiple predictor variables sometimes we want to understand which variable is the most influential in predicting the response (Y) variable. We can do this with varImp from the caret package. Here, we see that balance is the most important by a large margin whereas student status is less important followed by income (which was found to be insignificant anyways (p-value=0.81914):
```{r}
caret::varImp(model_2)
```
Let's examine a student's propensity to default:
```{r}
new.df <- tibble(balance = 1500, income = 40, student = c("Yes", "No"))
predict(model_2, new.df, type = "response")
```
Thus, we see that for the given balance and income (although income is insignificant) a student has about half the probability of defaulting than a non-student.


Note that, using polynomial transformations of predictors will allow a linear model to have non-linear decision boundaries.
```{r}
model_list <-  list(model_1, model_2, model_3)
train_errors <-  sapply(model_list, get_logistic_error, data = default_trn, 
                      res = "default", pos = "Yes", neg = "No", cut = 0.5)
test_errors  <-  sapply(model_list, get_logistic_error, data = default_tst, 
                      res = "default", pos = "Yes", neg = "No", cut = 0.5)
```

Here we see the misclassification error rates for each model. The train decreases, and the test decreases, until it starts to increases. Everything we learned about the bias-variance tradeoff for regression also applies here.
```{r}
diff(train_errors)
diff(test_errors)
```
### Model Evaluation and Diagnostics
Goodness-of-Fit
*Likelihood Ratio Test*
First, we can use a Likelihood Ratio Test to assess if our models are improving the fit. Adding predictor variables to a model will almost always improve the model fit (i.e. increase the log likelihood and reduce the model deviance compared to the null deviance), but it is necessary to test whether the observed difference in model fit is statistically significant. We can use anova to perform this test. 
```{r}
anova(model_glm, model_2, test = "Chisq")
```
The results indicate that, compared to model_glm (only balance predictor), model_2 reduces the residual deviance by over 21 (goal of logistic regression is to find a model that minimizes deviance residuals). More imporantly, this improvement is statisticallly significant at p = 2.095e-05. This suggests that model_2 (with all predictors) does provide an improved model fit.

*Pseudo R^2*
Unlike linear regression with ordinary least squares estimation, there is no R^2
statistic which explains the proportion of variance in the dependent variable that is explained by the predictors. However, there are a number of pseudo R^2
metrics that could be of value. Most notable is McFadden’s R^2, which is defined as
1-ln(LM_1)/ln(LM_0), where ln(LM_1) is a log likelihood of the fitted model and ln(LM_0) is a log likelihood of the null model with only intercept as a predictor.
```{r}
#install.packages("pscl")
library(pscl)
list(model_balance = pscl::pR2(model_glm)["McFadden"],
     model_all = pscl::pR2(model_2)["McFadden"])
```
Both models explain a fair amount of variance in the default data, model_all slightly improves the R^2 estimate.

#### Residual assessment
Logistic regression does not assume the residuals are normally distributed nor that the variance is constant. However, the deviance residual is useful for determining if individual points are not well fit by the model. Here we can fit the standardized deviance residuals to see how many exceed 3 standard deviations. First we extract several useful bits of model results with augment and then proceed to plot:
```{r}
model_2_data <- augment(model_2) %>% 
  mutate(index = 1:n())

ggplot(model_2_data, aes(index, .std.resid, color = default)) + 
  geom_point(alpha = .5) +
  geom_ref_line(h = 3)
```
Those standardized residuals that exceed 3 represent possible outliers and may deserve closer attention. We can filter for these residuals to get a closer look. We see that all these observations represent customers who defaulted with budgets that are much lower than the normal defaulters.
```{r}
model_2_data %>% 
  filter(abs(.std.resid) > 3)
model_2_data %>% summarise(Mean=mean(balance))
model_2_data %>% filter(default=="Yes") %>% summarise(Mean=mean(balance))


```
Similar to linear regression we can also identify influential observations with Cook’s distance values. Here we identify the top 5 largest values.
```{r}
plot(model_2, which = 4, id.n = 5)
# see https://stat.ethz.ch/R-manual/R-devel/library/stats/html/plot.lm.html for different plot calls with which
```
And we can investigate these further as well. Here we see that the top five influential points include those customers who defaulted with very low balances. Interestingly, no "influencers" were found among non-defaulters.
This means if we were to remove these observations (not recommended), the shape, location, and confidence interval of our logistic regression S-curve would likely shift.
```{r}
model_2_data %>% 
  top_n(5, .cooksd)
```



#### ROC curves
One predictor case
```{r}
model_glm <-  glm(default ~ balance, data = default_trn, family = "binomial")
```
Write a function which allows use to make predictions based on different probability cutoffs:
```{r}
get_logistic_pred <-  function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}
```

Let’s use this to obtain predictions using a low, medium, and high cutoff. (0.1, 0.5, and 0.9)
```{r}
test_pred_10 <-  get_logistic_pred(model_glm, data = default_tst, res = "default", 
                                 pos = "Yes", neg = "No", cut = 0.1)
test_pred_50 <-  get_logistic_pred(model_glm, data = default_tst, res = "default", 
                                 pos = "Yes", neg = "No", cut = 0.5)
test_pred_90 <-  get_logistic_pred(model_glm, data = default_tst, res = "default", 
                                 pos = "Yes", neg = "No", cut = 0.9)
```

Now we evaluate accuracy, sensitivity, and specificity for these classifiers.
```{r}
test_tab_10 <-  table(predicted = test_pred_10, actual = default_tst$default)
test_tab_50 <-  table(predicted = test_pred_50, actual = default_tst$default)
test_tab_90 <-  table(predicted = test_pred_90, actual = default_tst$default)

test_con_mat_10 <-  confusionMatrix(test_tab_10, positive = "Yes")
test_con_mat_50 <-  confusionMatrix(test_tab_50, positive = "Yes")
test_con_mat_90 <-  confusionMatrix(test_tab_90, positive = "Yes")
```
##goodmodel should have high sentivitity and specificality  (inclass)
```{r}
metrics <-  rbind(
  
  c(test_con_mat_10$overall["Accuracy"], 
    test_con_mat_10$byClass["Sensitivity"], 
    test_con_mat_10$byClass["Specificity"]),
  
  c(test_con_mat_50$overall["Accuracy"], 
    test_con_mat_50$byClass["Sensitivity"], 
    test_con_mat_50$byClass["Specificity"]),
  
  c(test_con_mat_90$overall["Accuracy"], 
    test_con_mat_90$byClass["Sensitivity"], 
    test_con_mat_90$byClass["Specificity"])

)

rownames(metrics) <-  c("c = 0.10", "c = 0.50", "c = 0.90")
metrics
```
We see then sensitivity decreases as the cutoff is increased. Conversely, specificity increases as the cutoff increases. This is useful if we are more interested in a particular error, instead of giving them equal weight.

**Note that usually the best accuracy will be seen near c=0.50.**

Instead of manually checking cutoffs, we can create a ROC curve (receiver operating characteristic curve) which will sweep through all possible cutoffs, and plot the sensitivity and specificity.
```{r}
#install.packages("pROC")   #receiver
library(pROC)
test_prob <-  predict(model_glm, newdata = default_tst, type = "response")
test_roc <-  roc(default_tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)
```

#
```{r}
#str(test_roc)
as.numeric(test_roc$auc)
```
A good model will have a high AUC, that is as often as possible a high sensitivity and specificity.

#### Repeat for other models
Both predictors   
```{r}
model_glm1 <-  glm(default ~ ., data = default_trn, family = "binomial")
test_prob <-  predict(model_glm1, newdata = default_tst, type = "response")
test_roc <-  roc(default_tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)
```
### Deafault ~ income
```{r}
model_glm2 <-  glm(default ~ income, data = default_trn, family = "binomial")
test_prob <-  predict(model_glm2, newdata = default_tst, type = "response")
test_roc <-  roc(default_tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)
```
#### A more complex model
```{r}
#model_3 <-  glm(default ~ . ^ 2 + I(balance ^ 2), data = default_trn, family = "binomial")
test_prob <-  predict(model_3, newdata = default_tst, type = "response")
test_roc <-  roc(default_tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)

```

###  Multinomial Logistic Regression
Using Iris dataset
Partition the data
```{r}
set.seed(2015)
iris_obs = nrow(iris)
iris_idx = sample(iris_obs, size = trunc(0.50 * iris_obs))
iris_trn = iris[iris_idx, ]
iris_test = iris[-iris_idx, ]
```
To perform multinomial logistic regression, we use the *multinom* function from the **nnet** package. Training using multinom() is done using similar syntax to lm() and glm(). We add the trace = FALSE argument to suppress information about updates to the optimization routine as the model is trained.
```{r}
#install.packages("nnet")
library(nnet)
model_multi = multinom(Species ~ ., data = iris_trn, trace = FALSE)
summary(model_multi)$coefficients
```
Notice we are only given coefficients for two of the three class, much like only needing coefficients for one class in logistic regression.


A difference between glm() and multinom() is how the predict() function operates:
```{r}
head(predict(model_multi, newdata = iris_trn))
head(predict(model_multi, newdata = iris_trn, type = "prob"))
```
Notice that by default, classifications are returned. When obtaining probabilities, we are given the predicted probability for each class.

We have just fit a neural network!

