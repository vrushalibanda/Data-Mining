
## Dealing with missing data with KNN:
### Packages and datasets
```{r}
#install.packages("tidyverse",repos = "http://cran.us.r-project.org")
#install.packages("AmesHousing", repos = "http://cran.us.r-project.org")
#install.packages(c("recipie", "caret"), repos = "http://cran.us.r-project.org")
library(AmesHousing)
library(tidyverse)

library(caret) # for various ML tasks
data(package="AmesHousing")
help("ames_schools")
dim(ames_raw)

```

### Visualizing missing data
It is important to understand the distribution of missing values in a data set in order to determine the best approach for preprocessing. Heat maps are an efficient way to visualize the distribution of missing values for small- to medium-sized data sets. The code is.na(<data-frame-name>) will return a matrix of the same dimension as the given data frame, but each cell will contain either TRUE (if the corresponding value is missing) or FALSE (if the corresponding value is not missing). To construct such a plot, we can use R’s built-in heatmap() or image() functions, or ggplot2’s geom_raster() function, among others;
```{r}
sum(is.na(ames_raw))

pdf("missing.pdf")
ames_raw %>%
  is.na() %>%
  reshape2::melt() %>%
  ggplot(aes(Var2, Var1, fill=value)) + 
    geom_raster() + 
    coord_flip() +
    scale_y_continuous(NULL, expand = c(0, 0)) +
    scale_fill_grey(name = "", 
                    labels = c("Present", 
                               "Missing")) +
    xlab("Observation") +
    theme(axis.text.y  = element_text(size = 4))
dev.off()
```
### Nature of the missing data
Digging a little deeper into these variables, we might notice that Garage_Cars and Garage_Area contain the value 0 whenever the other Garage_xx variables have missing values (i.e. a value of NA). This might be because they did not have a way to identify houses with no garages when the data were originally collected, and therefore, all houses with no garage were identified by including nothing. Since this missingness is informative, it would be appropriate to impute NA with a new category level (e.g., "None") for these garage variables. Circumstances like this tend to only become apparent upon careful descriptive and visual examination of the data!
```{r}
ames_raw %>% 
  filter(is.na(`Garage Type`)) %>% 
  select(`Garage Type`, `Garage Cars`, `Garage Area`)
```

### Alternative means of lookg at the missing data
The vis_miss() function in R package visdat also allows for easy visualization of missing data patterns (with sorting and clustering options). The columns of the heat map represent the 82 variables of the raw data and the rows represent the observations. Missing values (i.e., NA) are indicated via a black cell. The variables and NA patterns have been clustered by rows (i.e., cluster = TRUE).
```{r}
#install.packages("visdat")
library(visdat)
#pdf("miss_disdat.pdf")
vis_miss(ames_raw, cluster = TRUE)
#dev.off()
```
### Other parts of modelling process (a slightly mode systematic view)
Data splitting
Prerequisite libraries
```{r}
#install.packages(c("rsample", "h2o"))
# Helper packages
library(dplyr)     # for data manipulation
library(ggplot2)   # for awesome graphics

# Modeling process packages

library(rsample)   # for resampling procedures
library(caret)     # for resampling and model training
library(h2o)       # for resampling and model training
library(recipes) # for feature engineering tasks
# h2o set-up 
h2o.no_progress()  # turn off h2o progress bars
#h2o.init()         # launch h2o
```

### Make data
```{r}
h2o.init(startH2O = FALSE) 
ames <- AmesHousing::make_ames() # access data
ames.h2o <- as.h2o(ames)
dim(ames)
head(ames$Sale_Price)
```
### Different ways of data splitting
```{r}
# Using base R
set.seed(123)  # for reproducibility
index_1 <- sample(1:nrow(ames), round(nrow(ames) * 0.7))
train_1 <- ames[index_1, ]
test_1  <- ames[-index_1, ]

# Using caret package
set.seed(123)  # for reproducibility
index_2 <- createDataPartition(ames$Sale_Price, p = 0.7, 
                               list = FALSE)
train_2 <- ames[index_2, ]
test_2  <- ames[-index_2, ]

# Using rsample package
set.seed(123)  # for reproducibility
split_1  <- initial_split(ames, prop = 0.7)
train_3  <- training(split_1)
test_3   <- testing(split_1)

# Using h2o package
split_2 <- h2o.splitFrame(ames.h2o, ratios = 0.7, 
                          seed = 123)
train_4 <- split_2[[1]]
test_4  <- split_2[[2]]

# Stratified sampling with the rsample package
set.seed(123)
split <- initial_split(ames, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```


## Imputation
```{r}
library(recipes)
#View(ames$Gr_Liv_Area)
#help("recipes")
ames_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_log(all_outcomes())
ames_recipe %>%
  step_medianimpute(Gr_Liv_Area)

#Use step_modeimpute() to impute categorical features with the most common value.
```
## Imputation using K-nearest neighbor
k is a tunable hyperparameter. Suggested values for imputation are 5–10. By default, step_knnimpute() will use 5 but can be adjusted with the neighbors argument.
```{r}
ames_recipe %>%
  step_knnimpute(all_predictors(), neighbors = 6)
```
### Or bagged trees
```{r}
ames_recipe %>%
  step_bagimpute(all_predictors())
```

#K-nn modelling of house prices from Ames dataset
##Measuring distance
```{r}
(two_houses <- ames_train[1:2, c("Gr_Liv_Area", "Year_Built")])
# Euclidean
dist(two_houses, method = "euclidean")
# Manhattan
dist(two_houses, method = "manhattan")
```

## KNN binary data example
```{r}
#install.packages("ISLR")
#install.packages("class")
library(ISLR) #for student default dataset
library(class) #for doing knn classification
data(package="ISLR")
help("Default")
#View(Default)
```

###Variable conversion to numeric
knn() requires that all predictors be numeric, so we coerce student to be a 0 and 1 dummy variable instead of a factor. (We can, and should, leave the response as a factor.) Numeric predictors are required because of the distance calculations taking place.
```{r}
dim(Default)
set.seed(42)
Default$student = as.numeric(Default$student) - 1
default_idx = sample(nrow(Default), 5000)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]
head(Default$student)
```

knn() from class does not utilize the formula syntax, rather, requires the predictors be their own data frame or matrix, and the class labels be a separate factor variable. Note that the y data should be a factor vector, not a data frame containing a factor vector.
```{r}
# training data
head(default_trn)
X_default_trn = default_trn[, -1]
head(X_default_trn)
y_default_trn = default_trn$default

# testing data
X_default_tst = default_tst[, -1]
y_default_tst = default_tst$default
```
##Calssification
there is very little “training” with  
k-nearest neighbors. Essentially the only training is to simply remember the inputs. Because of this, we say that  k-nearest neighbors is fast at training time. However, at test time, k-nearest neighbors is very slow. For each test observation, the method must find the k-nearest neighbors, which is not computationally cheap. Note that by deafult, knn() uses Euclidean distance to determine neighbors.
```{r}
head(knn(train = X_default_trn, 
         test  = X_default_tst, 
         cl    = y_default_trn, 
         k     = 3))
```
Because of the lack of any need for training, the knn() function immediately returns classifications.
knn() takes four arguments:
train, the predictors for the train set.
test, the predictors for the test set. knn() will output results (classifications) for these cases.
cl, the true class labels for the train set.
k, the number of neighbors to consider.

###Evaluate classification erro
```{r}
calc_class_err <-  function(actual, predicted) {
  mean(actual != predicted)
}

calc_class_err(actual = y_default_tst,
               predicted = knn(train = X_default_trn,
                               test  = X_default_tst,
                               cl    = y_default_trn,
                               k     = 5))
```

###Scaling
Often with knn() consider the scale of the predictors variables. If one variable is contains much larger numbers because of the units or range of the variable, it will dominate other variables in the distance measurements. But this doesn’t necessarily mean that it should be such an important variable! It is common practice to scale the predictors to have a mean of zero and unit variance. Apply the scaling to both the train and test data!
Re-examine the accuracy of predictions
```{r}
calc_class_err(actual    = y_default_tst,
               predicted = knn(train = scale(X_default_trn), 
                               test  = scale(X_default_tst), 
                               cl    = y_default_trn, 
                               k     = 5))
```
Here the scaling slightly improves the classification accuracy. This may not always be the case, and often, it is normal to attempt classification with and without scaling.


##Choice of k
```{r}
set.seed(1984)
k_to_try = 1:100
err_k = rep(x = 0, times = length(k_to_try))

for (i in seq_along(k_to_try)) {
  pred = knn(train = scale(X_default_trn), 
             test  = scale(X_default_tst), 
             cl    = y_default_trn, 
             k     = k_to_try[i])
  err_k[i] = calc_class_err(y_default_tst, pred)
}
```
The seq_along() is very useful for looping over a vector that stores non-consecutive numbers. It removes the need for an additional counter variable. If you didn’t want to try every value of  k, but only odd integers, which would prevent ties. 

A seed before running this is because we are considering even values of  
k, thus, there are ties which are randomly broken.

### Plotting the results
```{r}
# plot error vs choice of k
plot(err_k, type = "b", col = "dodgerblue", cex = 1, pch = 20, 
     xlab = "k, number of neighbors", ylab = "classification error",
     main = "(Test) Error Rate vs Neighbors")
# add line for min error seen
abline(h = min(err_k), col = "darkorange", lty = 3)
# add line for minority prevalence in test set
abline(h = mean(y_default_tst == "Yes"), col = "grey", lty = 2)
```
The dotted orange line represents the smallest observed test classification error rate.
```{r}
min(err_k)
which(err_k == min(err_k))
```

Given a choice of these five values of  
k, we select the largest, as it is the **least variable**, and has the *least chance of overfitting*.
```{r}
max(which(err_k == min(err_k)))
```

NB:defaulters are the minority class. That is, the majority of observations are non-defaulters.
**As k increases, eventually the error approaches the test prevalence of the minority class.**
```{r}
table(y_default_tst)
mean(y_default_tst == "Yes")
```

## KNN on categorical data
KNN can be used for both binary and multi-class problems. As an example of a multi-class problems, we use iris data.
```{r}
 set.seed(430)
head(iris)
iris_obs = nrow(iris)
iris_idx = sample(iris_obs, size = trunc(0.50 * iris_obs))
iris_trn = iris[iris_idx, ]
iris_tst = iris[-iris_idx, ]
```
All the predictors here are numeric, so we proceed to splitting the data into predictors and classes.
```{r}
# training data
X_iris_trn = iris_trn[, -5]
y_iris_trn = iris_trn$Species

# testing data
X_iris_tst = iris_tst[, -5]
y_iris_tst = iris_tst$Species
```

Obtain predicted probabilities given test predictors by adding an argument: prob = TRUE
```{r}
iris_pred = knn(train = scale(X_iris_trn), 
                test  = scale(X_iris_tst),
                cl    = y_iris_trn,
                k     = 10,
                prob  = TRUE)
head(iris_pred, n=50)
```

Unfortunately, this only returns the predicted probability of the most common class. In the binary case, this would be sufficient to recover all probabilities, however, for multi-class problems, we cannot recover each of the probabilities of interest. 
```{r}
head(attributes(iris_pred)$prob, n = 50)
```

### caret package offers a framework for tuning a model
```{r}
library(caret)
default_knn_mod <-  train(
  default ~ .,
  data = default_trn,
  method = "knn",
  trControl = trainControl(method = "cv", number = 5)
)
```

Note that we are using formula syntax here, where previously we needed to create separate response and predictors matrices. Also, we’re using a factor variable as a predictor, and caret seems to be taking care of this automatically.
```{r}
default_knn_mod
```
Here we are again using 5-fold cross-validation and no pre-processing. Notice that we now have multiple results, for k = 5, k = 7, and k = 9.

Let’s modifying this training by introducing pre-processing, and specifying our own tuning parameters, instead of the default values above.
```{r}
default_knn_mod = train(
  default ~ .,
  data = default_trn,
  method = "knn",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(k = seq(1, 101, by = 2))
)
```
Here, we’ve specified that we would like to center and scale the data. Essentially transforming each predictor to have mean 0 and variance 1. The documentation on the preProcess() function provides examples of additional possible pre-processing. IN our call to train() we’re essentially specifying how we would like this function applied to our data.

We’ve also provided a “tuning grid,” in this case, the values of k to try. The tuneGrid argument expects a data frame, which expand.grid() returns. We don’t actually need expand.grid() for this example, but it will be a useful habit to develop when we move to methods with multiple tuning parameters.
```{r}
head(default_knn_mod$results, 5)
```


### Plotting results
```{r}
plot(default_knn_mod) #By default, caret utilizes the lattice graphics package
#or 
ggplot(default_knn_mod) + theme_bw() #wirh ggplot2 style graphics

```

### Best tuning parameter
Now that we are dealing with a tuning parameter, train() determines the best value of those considered, by default selecting the best (highest cross-validated) accuracy, and returning that value as bestTune.
```{r}
default_knn_mod$bestTune
```
Get all results from the best tune
```{r}
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
get_best_result(default_knn_mod)
```

While we did fit a large number of models, the “best” model is stored in finalModel. After this model was determined to be the best via cross-validation, it is then fit to the entire training dataset:
```{r}
default_knn_mod$finalModel
```
### Predict on the test set
```{r}
dim(default_trn)
head(predict(default_knn_mod, newdata = default_tst, type = "prob"), n=20)
```
### Multi-class response with iris data: all classes have assignes probs
```{r}
iris_knn_mod = train(
  Species ~ .,
  data = iris,
  method = "knn",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(k = seq(1, 21, by = 2))
)
head(predict(iris_knn_mod, type = "prob"), n =50)
```

### Statistics associated with tuning: accuracy and kappa
kappa: the amount of agreement corrected by the agreement expected by chance
```{r}
# build a starting dataframe
df <- data.frame(act = rep(LETTERS[1:2], each=10), pred = rep(sample(LETTERS[1:2], 20, replace=T)))
# create working frequency table
tab <- table(df)

# A balanced dataset
tab[1,1] <- 45
tab[1,2] <- 5
tab[2,1] <- 5
tab[2,2] <- 45

tab
caret::confusionMatrix(tab)

# An unbalanced datasest
tab[1,1] <- 85
tab[1,2] <- 5
tab[2,1] <- 5
tab[2,2] <- 5

caret::confusionMatrix(tab)
```
### Another KNN application
```{r}
#using employee attributes to predict the likelihood of attrition
#Employee attrition information originally provided by IBM Watson Analytics Lab
#response variable: Attrition (i.e., “Yes”, “No”)
#features: 30
#observations: 1,470
#objective: use employee attributes to predict if they will attrit (leave the company)
#access: provided by the rsample package

attrition <- rsample::attrition
dim(attrition)
head(attrition$Attrition)


```
###Create training set for attrition data
```{r}
# create training (70%) set for the rsample::attrition data.
library(rsample)
attrit <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
set.seed(123)
churn_split <- initial_split(attrit, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
```


### Create blueprint using *recipes* package
```{r}
library(recipes)
blueprint <- recipe(Attrition ~ ., data = churn_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(contains("Satisfaction")) %>%
  step_integer(WorkLifeBalance) %>%
  step_integer(JobInvolvement) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes())
```

### Creater a resampling method
```{r}
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5,
  classProbs = TRUE,                 
  summaryFunction = twoClassSummary
)
```

### Create a hyperparameter grid search
```{r}
hyper_grid <- expand.grid(
  k = floor(seq(1, nrow(churn_train)/3, length.out = 20))
)
```

### Fit knn model and perform grid search and plot
```{r}
knn_grid <- train(
  blueprint, 
  data = churn_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "ROC"
)

ggplot(knn_grid)
```

