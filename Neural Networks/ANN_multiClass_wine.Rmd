

#### Prereqs
```{r}
library(neuralnet)
library(nnet)
library(tidyverse)
```

#### Data
```{r}
rm(list=ls())
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wines <- read_csv(file = url,
                  col_names = c("label",
                  "Alcohol",
                  "Malic_acid",
                  "Ash",
                  "Alcalinity_of_ash",
                  "Magnesium",
                  "Total_phenols",
                  "Flavanoids",
                  "Nonflavanoid_phenols",
                  "Proanthocyanins",
                  "Color_intensity",
                  "Hue",
                  "OD280_OD315_of_diluted_wines",
                  "Proline"))
head(wines)
```
PLot some variables
```{r}
plt1 <- ggplot(wines, aes(x = Alcohol, y = Magnesium, colour = as.factor(label))) +
    geom_point(size=3) +
    ggtitle("Wines")
plt2 <- ggplot(wines, aes(x = Alcohol, y = Proline, colour = as.factor(label))) +
    geom_point(size=3) +
    ggtitle("Wines")
plt1
plt2
```
#### Encode as a one hot vector multilabel data
```{r}
train <- cbind(wines[, 2:14], class.ind(as.factor(wines$label)))
# Set labels name
names(train) <- c(names(wines)[2:14],"l1","l2","l3")
head(train)
```

#### Scale
```{r}
scl <- function(x){ (x - min(x))/(max(x) - min(x)) }
train[, 1:13] <- data.frame(lapply(train[, 1:13], scl))
head(train)
```
#### Fitting the model with neuralnet
```{r}
nn <- neuralnet(l1+l2+l3 ~.,
                data = train,
                hidden = c(13, 10, 3),
                act.fct = "logistic",
                linear.output = FALSE,
                err.fct = "ce",
                likelihood = T,
                lifesign = "minimal")
```
And plot
```{r}
plot(nn)
```
too busy...

#### Accuracy on the training set
```{r}
# Compute predictions
pr.nn <- compute(nn, train[, 1:13])
# Extract results
pr.nn_ <- pr.nn$net.result
head(pr.nn_)
# Accuracy (training set)
original_values <- max.col(train[, 14:16])
pr.nn_2 <- max.col(pr.nn_)
mean(pr.nn_2 == original_values)
```

The accuracy on the training set is 100!

#### Cross-validation of the classifier
Letâ€™s crossvalidate the model using the evergreen 10 fold cross validation with the following train and test split: 95% of the dataset will be used as training set while the remaining 5% as test set.
```{r}
# Set seed for reproducibility purposes
set.seed(123123)
# 10 fold cross validation
k <- 10
# Results from cv
outs <- NULL
# Train test split proportions
proportion <- 0.95 # Set to 0.995 for LOOCV, set the proportion variable to 0.995: this will select just one observation for as test set and leave all the other observations as training set. 

# Crossvalidate,:
for(i in 1:k)
{
    index <- sample(1:nrow(train), round(proportion*nrow(train)))
    train_cv <- train[index, ]
    test_cv <- train[-index, ]
    nn_cv <- neuralnet(l1+l2+l3 ~.,
                        data = train_cv,
                        hidden = c(13, 10, 3),
                        act.fct = "logistic",
                        linear.output = FALSE)
    
    # Compute predictions
    pr.nn <- compute(nn_cv, test_cv[, 1:13])
    # Extract results
    pr.nn_ <- pr.nn$net.result
    # Accuracy (test set)
    original_values <- max.col(test_cv[, 14:16])
    pr.nn_2 <- max.col(pr.nn_)
    outs[i] <- mean(pr.nn_2 == original_values)
}

mean(outs)
```


